{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6c8ad488-3eae-4bd9-aec2-d3b308224dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "import re\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331398a4-4f01-4fb0-8e09-5aafdf6fa3f6",
   "metadata": {},
   "source": [
    "## read data set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "84454f22-8f52-4a57-8e50-ba8d2db40b79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>spam</td>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1                                                 v2 Unnamed: 2  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
       "1   ham                      Ok lar... Joking wif u oni...        NaN   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
       "3   ham  U dun say so early hor... U c already then say...        NaN   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
       "5  spam  FreeMsg Hey there darling it's been 3 week's n...        NaN   \n",
       "6   ham  Even my brother is not like to speak with me. ...        NaN   \n",
       "\n",
       "  Unnamed: 3 Unnamed: 4  \n",
       "0        NaN        NaN  \n",
       "1        NaN        NaN  \n",
       "2        NaN        NaN  \n",
       "3        NaN        NaN  \n",
       "4        NaN        NaN  \n",
       "5        NaN        NaN  \n",
       "6        NaN        NaN  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data= pd.read_csv('spam.csv.csv', encoding=\"ISO-8859-1\")\n",
    "data.head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a377dd-6f10-4fb6-a1b7-b1df965b7dd5",
   "metadata": {},
   "source": [
    "# to remove URL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ef9aa2b5-2337-43d9-a447-1110b2f734e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_url(text1):\n",
    "    return re.sub(r'http\\S+', ' ', text1)\n",
    "data['ClaenEmails']= data['v2'].apply(clean_url)\n",
    "text1=data['ClaenEmails'][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "313a10dd-3dae-4bb0-91f2-d32bd6219f80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Go until jurong point, crazy.. Available only ...\n",
       "1                        Ok lar... Joking wif u oni...\n",
       "2    Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3    U dun say so early hor... U c already then say...\n",
       "4    Nah I don't think he goes to usf, he lives aro...\n",
       "5    FreeMsg Hey there darling it's been 3 week's n...\n",
       "6    Even my brother is not like to speak with me. ...\n",
       "7    As per your request 'Melle Melle (Oru Minnamin...\n",
       "8    WINNER!! As a valued network customer you have...\n",
       "9    Had your mobile 11 months or more? U R entitle...\n",
       "Name: ClaenEmails, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1b4766e9-7543-4132-9847-1299e33837e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "      <th>ClaenEmails</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will Ì_ b going to esplanade fr home?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Will Ì_ b going to esplanade fr home?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        v1                                                 v2 Unnamed: 2  \\\n",
       "0      ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
       "1      ham                      Ok lar... Joking wif u oni...        NaN   \n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
       "3      ham  U dun say so early hor... U c already then say...        NaN   \n",
       "4      ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
       "...    ...                                                ...        ...   \n",
       "5567  spam  This is the 2nd time we have tried 2 contact u...        NaN   \n",
       "5568   ham              Will Ì_ b going to esplanade fr home?        NaN   \n",
       "5569   ham  Pity, * was in mood for that. So...any other s...        NaN   \n",
       "5570   ham  The guy did some bitching but I acted like i'd...        NaN   \n",
       "5571   ham                         Rofl. Its true to its name        NaN   \n",
       "\n",
       "     Unnamed: 3 Unnamed: 4                                        ClaenEmails  \n",
       "0           NaN        NaN  Go until jurong point, crazy.. Available only ...  \n",
       "1           NaN        NaN                      Ok lar... Joking wif u oni...  \n",
       "2           NaN        NaN  Free entry in 2 a wkly comp to win FA Cup fina...  \n",
       "3           NaN        NaN  U dun say so early hor... U c already then say...  \n",
       "4           NaN        NaN  Nah I don't think he goes to usf, he lives aro...  \n",
       "...         ...        ...                                                ...  \n",
       "5567        NaN        NaN  This is the 2nd time we have tried 2 contact u...  \n",
       "5568        NaN        NaN              Will Ì_ b going to esplanade fr home?  \n",
       "5569        NaN        NaN  Pity, * was in mood for that. So...any other s...  \n",
       "5570        NaN        NaN  The guy did some bitching but I acted like i'd...  \n",
       "5571        NaN        NaN                         Rofl. Its true to its name  \n",
       "\n",
       "[5572 rows x 6 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd89fc20-3e8f-4699-b554-8e723f1f7f3d",
   "metadata": {},
   "source": [
    "# To remove Numbers and Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "64e3ce02-b820-43c6-a008-e446edfedf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_non_alphanumeric(text2):\n",
    "    return re.sub('[^a-zA-Z]',' ',text2)\n",
    "data['CleanEmails']=data['v2'].apply(clean_non_alphanumeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "17059243-e14d-481f-b27f-bbf48ba2f902",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "      <th>ClaenEmails</th>\n",
       "      <th>CleanEmails</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>Go  jurong  point  crazy  ..  Available  bugis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>Ok  lar  ...  Joking  wif  oni  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>Free  entry  wkly  comp  win  FA  Cup  final  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>dun  say  early  hor  ...  already  say  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>Nah  n't  think  go  usf  live  around  though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1                                                 v2 Unnamed: 2  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
       "1   ham                      Ok lar... Joking wif u oni...        NaN   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
       "3   ham  U dun say so early hor... U c already then say...        NaN   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
       "\n",
       "  Unnamed: 3 Unnamed: 4                                        ClaenEmails  \\\n",
       "0        NaN        NaN  Go until jurong point, crazy.. Available only ...   \n",
       "1        NaN        NaN                      Ok lar... Joking wif u oni...   \n",
       "2        NaN        NaN  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3        NaN        NaN  U dun say so early hor... U c already then say...   \n",
       "4        NaN        NaN  Nah I don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                         CleanEmails  \n",
       "0  Go  jurong  point  crazy  ..  Available  bugis...  \n",
       "1                Ok  lar  ...  Joking  wif  oni  ...  \n",
       "2  Free  entry  wkly  comp  win  FA  Cup  final  ...  \n",
       "3       dun  say  early  hor  ...  already  say  ...  \n",
       "4     Nah  n't  think  go  usf  live  around  though  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7013a6-6535-4ad3-af9f-eb6e64c8eb57",
   "metadata": {},
   "source": [
    "## remove white space "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "55f5101d-d307-4537-a05c-68889662fe72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_space(text3):\n",
    "    return re.sub(r'\\s+', ' ',text3)\n",
    "data['CleanEmails'] = data['v2'].apply(clear_space)\n",
    "\n",
    "text3=data['CleanEmails'][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "77a17a62-f849-45ec-b371-57c48a8720ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Go until jurong point, crazy.. Available only ...\n",
       "1                        Ok lar... Joking wif u oni...\n",
       "2    Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3    U dun say so early hor... U c already then say...\n",
       "4    Nah I don't think he goes to usf, he lives aro...\n",
       "5    FreeMsg Hey there darling it's been 3 week's n...\n",
       "6    Even my brother is not like to speak with me. ...\n",
       "7    As per your request 'Melle Melle (Oru Minnamin...\n",
       "8    WINNER!! As a valued network customer you have...\n",
       "9    Had your mobile 11 months or more? U R entitle...\n",
       "Name: CleanEmails, dtype: object"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c659c4-93c1-475e-af3c-ecd0379e30d1",
   "metadata": {},
   "source": [
    "## tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0c9fb7d3-ce0e-4e06-ae37-db2ec7f3e9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "841135fb-6e67-4bc8-a903-7a01ec15cae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Download the 'punkt' resource\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "19ba3a03-a700-4450-b43c-f41caffa0fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text4):\n",
    "    return word_tokenize(text4)\n",
    "\n",
    "data['CleanEmails']=data['CleanEmails'].apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5889ddcc-6c94-4ff8-93e9-709cdf53cd73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [Go, until, jurong, point, ,, crazy, .., Avail...\n",
       "1                [Ok, lar, ..., Joking, wif, u, oni, ...]\n",
       "2       [Free, entry, in, 2, a, wkly, comp, to, win, F...\n",
       "3       [U, dun, say, so, early, hor, ..., U, c, alrea...\n",
       "4       [Nah, I, do, n't, think, he, goes, to, usf, ,,...\n",
       "                              ...                        \n",
       "5567    [This, is, the, 2nd, time, we, have, tried, 2,...\n",
       "5568     [Will, Ì_, b, going, to, esplanade, fr, home, ?]\n",
       "5569    [Pity, ,, *, was, in, mood, for, that, ., So, ...\n",
       "5570    [The, guy, did, some, bitching, but, I, acted,...\n",
       "5571                  [Rofl, ., Its, true, to, its, name]\n",
       "Name: CleanEmails, Length: 5572, dtype: object"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['CleanEmails']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8840af9e-59c9-4b9f-aebc-6365b15bc31a",
   "metadata": {},
   "source": [
    "## find stop word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7d5219b0-3dc3-47fe-bbb1-e622b5a1faab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a13276c6-12c1-4c5a-bdb3-537cefa45942",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words=set(stopwords.words('english'))\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1725acb-e1d0-4ae6-bb0f-10e12b0a1c43",
   "metadata": {},
   "source": [
    "## remove stop word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c0dc9f5c-912b-4400-8b93-4b78c60aa4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_stopword(token):\n",
    "    return [item for item in token if item not in stop_words]\n",
    "data['CleanEmails']=data['CleanEmails'].apply(clean_stopword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "009a14a7-039a-48bd-886b-a646e678cc38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7b6fa294-c21e-4554-a4a1-f3d90ad8b7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4d7a3508-d08c-4bdd-81b0-ef1c414e7c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "lem=WordNetLemmatizer()\n",
    "\n",
    "def clean_lemmatization(token):\n",
    "    return[lem.lemmatize(word=w,pos='v') for w in token]\n",
    "\n",
    "data['CleanEmails']=data['CleanEmails'].apply(clean_lemmatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "becd81fc-4d29-48c4-a51c-1a1336896d3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [Go, jurong, point, ,, crazy, .., Available, b...\n",
       "1                [Ok, lar, ..., Joking, wif, u, oni, ...]\n",
       "2       [Free, entry, 2, wkly, comp, win, FA, Cup, fin...\n",
       "3       [U, dun, say, early, hor, ..., U, c, already, ...\n",
       "4       [Nah, I, n't, think, go, usf, ,, live, around,...\n",
       "                              ...                        \n",
       "5567    [This, 2nd, time, try, 2, contact, u., U, å£75...\n",
       "5568            [Will, Ì_, b, go, esplanade, fr, home, ?]\n",
       "5569       [Pity, ,, *, mood, ., So, ..., suggestions, ?]\n",
       "5570    [The, guy, bitch, I, act, like, 'd, interest, ...\n",
       "5571                           [Rofl, ., Its, true, name]\n",
       "Name: CleanEmails, Length: 5572, dtype: object"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['CleanEmails']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492316ae-772a-4e6d-a227-4ae18b5ae6f3",
   "metadata": {},
   "source": [
    "## Remove meaningless characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "068e9f05-6d3e-4c5a-85ff-e170797d3935",
   "metadata": {},
   "outputs": [],
   "source": [
    "CleanEmailText=[]\n",
    "def clean_lenght(token):\n",
    "    filtered_tokens=[]\n",
    "    for i in token:\n",
    "        if len(i)>=2:\n",
    "            filtered_tokens.append(i)\n",
    "    return filtered_tokens\n",
    "data['CleanEmails']=data['CleanEmails'].apply(clean_lenght)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f13cf333-ad39-4430-99a5-746dc88285f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [Go, jurong, point, crazy, .., Available, bugi...\n",
       "1                   [Ok, lar, ..., Joking, wif, oni, ...]\n",
       "2       [Free, entry, wkly, comp, win, FA, Cup, final,...\n",
       "3          [dun, say, early, hor, ..., already, say, ...]\n",
       "4        [Nah, n't, think, go, usf, live, around, though]\n",
       "                              ...                        \n",
       "5567    [This, 2nd, time, try, contact, u., å£750, Pou...\n",
       "5568                  [Will, Ì_, go, esplanade, fr, home]\n",
       "5569                   [Pity, mood, So, ..., suggestions]\n",
       "5570    [The, guy, bitch, act, like, 'd, interest, buy...\n",
       "5571                              [Rofl, Its, true, name]\n",
       "Name: CleanEmails, Length: 5572, dtype: object"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['CleanEmails']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b32079-3560-439a-bb7d-82ece8d1a6b3",
   "metadata": {},
   "source": [
    "## convert list of words back to string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ccfbeec9-ac2f-4ebe-ae0d-a682d27b7ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_string(listReview):\n",
    "    return '  '.join(listReview)\n",
    "\n",
    "data['CleanEmails']=data['CleanEmails'].apply(convert_to_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8770d42a-6678-4049-9f95-ae0d322e7196",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"FreeMsg  Hey  darling  's  week  's  word  back  'd  like  fun  still  Tb  ok  XxX  std  chgs  send  å£1.50  rcv\""
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['CleanEmails'][5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61144e23-8a4c-43a7-9696-37f2c4093436",
   "metadata": {},
   "source": [
    "# Question 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12196be-b14c-499e-a1bb-789692c54fd6",
   "metadata": {},
   "source": [
    "## Splitting data into training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d6c76db9-fc83-4481-982e-a8ff93724c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = data['v2']\n",
    "X = data['CleanEmails']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7753911e-0997-4eaf-a6a2-f018d612dd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,Y,test_size = 0.3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0951804-15eb-4b9c-9777-48e0d9e8039a",
   "metadata": {},
   "source": [
    "## Extract Features using TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "64d3097c-b464-484b-a26f-41a29d3622a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['00', '000', '008704050406', ..., 'ûªve', 'ûï', 'ûò'], dtype=object)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)\n",
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bdb72e-02f3-483d-9c39-ec403874f440",
   "metadata": {},
   "source": [
    "### Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1cef49f1-a007-4469-b0fe-393c626c7d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.07954545454545454\n"
     ]
    }
   ],
   "source": [
    "model = svm.SVC()\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Accuracy :\", model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b374324d-643a-4c53-a0ae-1a8812f330da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1a5c0a-a59a-4328-82fe-1334ce793bf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e2901f-1be2-4525-b75f-f97b0ebf5d78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6d5531-3939-4623-99b1-c44589b2a31e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
